{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SE6qcw_j8Pi2"
   },
   "source": [
    "In this exercise, you are requested to implement a Neural Network with backpropogation using only *numpy*.\n",
    "\n",
    "Notice:\n",
    "- All activation functions in this network are Sigmoid\n",
    "- The output of the network is one single value\n",
    "\n",
    "### Created by Omer Keren & Alon Albahari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "UV4RvXYL8k85"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "kLdNoCt58qg5"
   },
   "outputs": [],
   "source": [
    "class MyNN:\n",
    "    def __init__(self, learning_rate, layer_sizes):\n",
    "        '''\n",
    "        learning_rate - remember etta from class? this is it!\n",
    "        layer_sizes - a simple list of numbers - where each number is the size of each layer - i.e. the\n",
    "                        number of neurons in the layer. So the length of this list is the number of layers\n",
    "                        in our neural network\n",
    "        model_param - a dictionary that will store the parameters/variables - such as weights and biases\n",
    "        '''\n",
    "        self.learning_rate = learning_rate\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.model_params = {}\n",
    "        self.memory = {}\n",
    "        self.grads = {}\n",
    "\n",
    "        # Initializing weights with random values\n",
    "        for layer_index in range(len(layer_sizes) - 1):\n",
    "            W_input = layer_sizes[layer_index + 1]\n",
    "            W_output = layer_sizes[layer_index]\n",
    "            self.model_params['W_' + str(layer_index + 1)] = np.random.randn(W_input, W_output) * 0.1\n",
    "            self.model_params['b_' + str(layer_index + 1)] = np.random.randn(W_input) * 0.1\n",
    "\n",
    "    # this function performs forward step (over all network elements) for a single data item/instance \n",
    "    def forward_single_instance(self, x):    \n",
    "        a_i_1 = x\n",
    "        self.memory['a_0'] = x\n",
    "        for layer_index in range(len(self.layer_sizes) - 1):\n",
    "            W_i = self.model_params['W_' + str(layer_index + 1)]\n",
    "            b_i = self.model_params['b_' + str(layer_index + 1)]\n",
    "            z_i = np.dot(W_i, a_i_1) + b_i\n",
    "            a_i = 1/(1+np.exp(-z_i))\n",
    "            self.memory['a_' + str(layer_index + 1)] = a_i\n",
    "            a_i_1 = a_i\n",
    "        return a_i_1\n",
    "  \n",
    "    # a sample loss function\n",
    "    def log_loss(y_hat, y):\n",
    "        '''\n",
    "        Logistic loss, assuming a single value in y_hat and y.\n",
    "        '''\n",
    "        m = y_hat[0]\n",
    "        cost = -y[0]*np.log(y_hat[0]) - (1 - y[0])*np.log(1 - y_hat[0])\n",
    "        return cost\n",
    "  \n",
    "  \n",
    "  #this function performs a backpropogation for a single data item/instance\n",
    "    def backward_single_instance(self, y):\n",
    "        a_output = self.memory['a_' + str(len(self.layer_sizes) - 1)]\n",
    "        dz = a_output - y\n",
    "\n",
    "        for layer_index in range(len(self.layer_sizes) - 1, 0, -1):\n",
    "            print(layer_index)\n",
    "            # means activations functions for each position in the network\n",
    "            a_l_1 = self.memory['a_' + str(layer_index - 1)]\n",
    "            # error of prediction * res\n",
    "            dW = np.dot(dz.reshape(-1, 1), a_l_1.reshape(1, -1))\n",
    "            self.grads['dW_' + str(layer_index)] = dW\n",
    "            # current wieght\n",
    "            W_l = self.model_params['W_' + str(layer_index)]\n",
    "            # current bias\n",
    "            db = dz\n",
    "            self.grads['db_' + str(layer_index)] = db\n",
    "            dz = (a_l_1 * (1 - a_l_1)).reshape(-1, 1) * np.dot(W_l.T, dz.reshape(-1, 1))\n",
    "\n",
    "    def update(self):\n",
    "        for layer_index in range(len(self.layer_sizes) - 1, 0, -1):\n",
    "            # names of key objects\n",
    "            Wi = 'W_' + str(layer_index)\n",
    "            bi = 'b_' + str(layer_index)\n",
    "            dWi = 'dW_' + str(layer_index)\n",
    "            dbi = 'db_' + str(layer_index)\n",
    "            \n",
    "            W_i = self.model_params[Wi]\n",
    "            b_i = self.model_params[bi]\n",
    "            dW_i = self.grads[dWi]\n",
    "            db_i = self.grads[dbi]\n",
    "            \n",
    "            self.model_params[Wi] = W_i - self.learning_rate * dW_i\n",
    "            self.model_params[bi] = b_i - self.learning_rate * db_i\n",
    "\n",
    " \n",
    "      #def forward_batch(self, X)\n",
    "    def forward_batch(self, x):    \n",
    "        a_i_1 = x\n",
    "        self.memory['a_0'] = x\n",
    "        for layer_index in range(len(self.layer_sizes) - 1):\n",
    "            W_i = self.model_params['W_' + str(layer_index + 1)]\n",
    "            b_i = self.model_params['b_' + str(layer_index + 1)]\n",
    "            z_i = np.dot(W_i, a_i_1) + b_i.reshape(-1,1)\n",
    "            a_i = 1/(1+np.exp(-z_i))\n",
    "            self.memory['a_' + str(layer_index + 1)] = a_i\n",
    "            a_i_1 = a_i\n",
    "        return a_i_1\n",
    "            \n",
    "\n",
    "      # TODO: implement backward for a batch y.shape = (1, number_of_instance)\n",
    "      #def backward_batch(self, y)\n",
    "    def backward_batch(self, y):\n",
    "        m = y.shape[1]\n",
    "        a_output = self.memory['a_' + str(len(self.layer_sizes) - 1)]\n",
    "        dz = a_output - y\n",
    "        \n",
    "        for layer_index in range(len(self.layer_sizes) - 1, 0, -1):\n",
    "            # means activations functions for each position in the network\n",
    "            a_l_1 = self.memory['a_' + str(layer_index - 1)]\n",
    "\n",
    "            # error of prediction * res\n",
    "            dW = (np.dot(dz, a_l_1.T)) / m\n",
    "            self.grads['dW_' + str(layer_index)] = dW\n",
    "\n",
    "            # current wieght\n",
    "            W_l = self.model_params['W_' + str(layer_index)]\n",
    "\n",
    "            # (activations vector devrsion) * (current wieght)\n",
    "            tmp = dz\n",
    "            self.memory['dz_' + str(layer_index)] = dz\n",
    "\n",
    "            # current bias\n",
    "            b_l = self.model_params['b_' + str(layer_index)]\n",
    "\n",
    "            # (activations vector devrsion) * (current bias)\n",
    "            db = np.sum(dz) / m\n",
    "            self.grads['db_' + str(layer_index)] = db\n",
    "            dz = (a_l_1 * (1 - a_l_1)) * np.dot(W_l.T, tmp)\n",
    "\n",
    "            \n",
    "\n",
    "      # TODO: implement log_loss_batch, for a batch of instances\n",
    "      # def log_loss(self, y_hat, y)\n",
    "    def log_loss_batch(self, y_hat, y):\n",
    "        m = y_hat.shape[1]\n",
    "        cost = 1/m * (-np.dot(y,np.log(y_hat).T) - np.dot(1-y,np.log(1-y_hat).T))\n",
    "        return cost\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SRml6glFIPCa"
   },
   "source": [
    "The next class will cover the neural network. We will focus on what we learned in class - A feed forward network having multiple layers.\n",
    "The class already has methods for forward and backward runs of a single instance of data (single row in a dataset). You have to implement each code that has \"TODO\" in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "Qib6W4QXO644"
   },
   "outputs": [],
   "source": [
    "nn = MyNN(0.01, [3, 2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "id": "4nQR8QllPf_5",
    "outputId": "c63240fd-e28e-41e4-f7f2-7263675c01b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W_1': array([[-0.00183457, -0.00417865, -0.04180828],\n",
       "        [ 0.08847231,  0.07212246,  0.11498373]]),\n",
       " 'b_1': array([ 0.13839357, -0.00715556]),\n",
       " 'W_2': array([[-0.10545561, -0.00624427]]),\n",
       " 'b_2': array([-0.11271583])}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "VXiyn-yrPC6-",
    "outputId": "4de9d376-b761-40db-eb08-271ea57ff0e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.45695926]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(3)\n",
    "y = np.random.randn(1)\n",
    "\n",
    "y_hat = nn.forward_single_instance(x)\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "id": "k5M50i3plclj",
    "outputId": "aff5d3ea-d5ee-47eb-9db9-65cda7e5b4b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "nn.backward_single_instance(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "CWnZB1YmYnIt"
   },
   "outputs": [],
   "source": [
    "# A train function for training the neural network\n",
    "def train(X, y, epochs, batch_size):\n",
    "    lossHistory = np.zeros((epochs,1))\n",
    "    for e in range(1, epochs + 1):\n",
    "        epoch_loss = 0\n",
    "        # shuffle\n",
    "        shuffler = np.random.permutation(X.shape[1])\n",
    "        X = X[:,shuffler]\n",
    "        y = y[:,shuffler]\n",
    "        # divide to batches\n",
    "        X = X[:,0:(X.shape[1]//batch_size)*batch_size]\n",
    "        y = y[:,0:(y.shape[1]//batch_size)*batch_size]\n",
    "        batches = np.array(list(zip(np.split(X,X.shape[1]//batch_size,axis=1),np.split(y,y.shape[1]//batch_size,axis=1)))) \n",
    "\n",
    "        for X_b, y_b in batches:\n",
    "            #print(X_b)\n",
    "            #print(y_b)\n",
    "            y_hat = nn.forward_batch(X_b)\n",
    "            epoch_loss += nn.log_loss_batch(y_hat, y_b)\n",
    "            nn.backward_batch(y_b)\n",
    "            nn.update()\n",
    "            lossHistory[e-1] = epoch_loss/len(batches)\n",
    "\n",
    "        print(f'Epoch {e}, loss={epoch_loss/len(batches)}')\n",
    "    return lossHistory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "cE1ydWlatkty",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss=[[0.73440766]]\n",
      "Epoch 2, loss=[[0.73090487]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-103-3e21d7ec8c41>:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  batches = np.array(list(zip(np.split(X,X.shape[1]//batch_size,axis=1),np.split(y,y.shape[1]//batch_size,axis=1))))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.73440766],\n",
       "       [0.73090487]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Make sure the following network trains properly\n",
    "\n",
    "nn = MyNN(0.001, [6, 4, 3, 1])\n",
    "\n",
    "X = np.random.randn(6, 100)\n",
    "y = np.random.randn(1, 100)\n",
    "batch_size = 8\n",
    "epochs = 2\n",
    "\n",
    "train(X, y, epochs, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6dY4scUksulC"
   },
   "source": [
    "#TODO: train on an external dataset\n",
    "\n",
    "Train on the Bike Sharing dataset, using day.csv file from https://archive.ics.uci.edu/ml/machine-learning-databases/00275/.\n",
    "Use the following features from the data:\n",
    "\n",
    "* temp\n",
    "* atemp\n",
    "* hum\n",
    "* windspeed\n",
    "* weekday\n",
    "\n",
    "The response variable is as the following:\n",
    "    raw[\"success\"] = raw[\"cnt\"] > (raw[\"cnt\"].describe()[\"mean\"]).\n",
    "\n",
    "The architecture of the network should be: [5, 40, 30, 10, 7, 5, 3, 1].\n",
    "\n",
    "Use batch_size=8, and train it for 100 epochs on the train set (based on the split as requested above).\n",
    "\n",
    "Then, plot loss per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "3Y0GvPArZear"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-103-3e21d7ec8c41>:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  batches = np.array(list(zip(np.split(X,X.shape[1]//batch_size,axis=1),np.split(y,y.shape[1]//batch_size,axis=1))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss=[[0.6958921]]\n",
      "Epoch 2, loss=[[0.69567163]]\n",
      "Epoch 3, loss=[[0.69546472]]\n",
      "Epoch 4, loss=[[0.69529203]]\n",
      "Epoch 5, loss=[[0.69510478]]\n",
      "Epoch 6, loss=[[0.69494988]]\n",
      "Epoch 7, loss=[[0.69480605]]\n",
      "Epoch 8, loss=[[0.69467503]]\n",
      "Epoch 9, loss=[[0.69453867]]\n",
      "Epoch 10, loss=[[0.69443459]]\n",
      "Epoch 11, loss=[[0.69432395]]\n",
      "Epoch 12, loss=[[0.69421709]]\n",
      "Epoch 13, loss=[[0.69413803]]\n",
      "Epoch 14, loss=[[0.69405071]]\n",
      "Epoch 15, loss=[[0.69397814]]\n",
      "Epoch 16, loss=[[0.69390298]]\n",
      "Epoch 17, loss=[[0.69383327]]\n",
      "Epoch 18, loss=[[0.69377672]]\n",
      "Epoch 19, loss=[[0.69371711]]\n",
      "Epoch 20, loss=[[0.69366466]]\n",
      "Epoch 21, loss=[[0.69361533]]\n",
      "Epoch 22, loss=[[0.69356821]]\n",
      "Epoch 23, loss=[[0.69353557]]\n",
      "Epoch 24, loss=[[0.69349081]]\n",
      "Epoch 25, loss=[[0.69345619]]\n",
      "Epoch 26, loss=[[0.69342644]]\n",
      "Epoch 27, loss=[[0.69340263]]\n",
      "Epoch 28, loss=[[0.69336849]]\n",
      "Epoch 29, loss=[[0.69334329]]\n",
      "Epoch 30, loss=[[0.69331705]]\n",
      "Epoch 31, loss=[[0.69329908]]\n",
      "Epoch 32, loss=[[0.69328235]]\n",
      "Epoch 33, loss=[[0.69325619]]\n",
      "Epoch 34, loss=[[0.69324138]]\n",
      "Epoch 35, loss=[[0.69322928]]\n",
      "Epoch 36, loss=[[0.69321216]]\n",
      "Epoch 37, loss=[[0.69319434]]\n",
      "Epoch 38, loss=[[0.69318294]]\n",
      "Epoch 39, loss=[[0.69317755]]\n",
      "Epoch 40, loss=[[0.69316253]]\n",
      "Epoch 41, loss=[[0.69315616]]\n",
      "Epoch 42, loss=[[0.69314912]]\n",
      "Epoch 43, loss=[[0.69313699]]\n",
      "Epoch 44, loss=[[0.69312816]]\n",
      "Epoch 45, loss=[[0.69312654]]\n",
      "Epoch 46, loss=[[0.69311533]]\n",
      "Epoch 47, loss=[[0.69311519]]\n",
      "Epoch 48, loss=[[0.69310897]]\n",
      "Epoch 49, loss=[[0.69310554]]\n",
      "Epoch 50, loss=[[0.69309859]]\n",
      "Epoch 51, loss=[[0.69308928]]\n",
      "Epoch 52, loss=[[0.69308773]]\n",
      "Epoch 53, loss=[[0.69308328]]\n",
      "Epoch 54, loss=[[0.69308198]]\n",
      "Epoch 55, loss=[[0.69307564]]\n",
      "Epoch 56, loss=[[0.69307884]]\n",
      "Epoch 57, loss=[[0.69306509]]\n",
      "Epoch 58, loss=[[0.69306447]]\n",
      "Epoch 59, loss=[[0.69307216]]\n",
      "Epoch 60, loss=[[0.69305864]]\n",
      "Epoch 61, loss=[[0.69305951]]\n",
      "Epoch 62, loss=[[0.69305344]]\n",
      "Epoch 63, loss=[[0.69305567]]\n",
      "Epoch 64, loss=[[0.69306396]]\n",
      "Epoch 65, loss=[[0.69305661]]\n",
      "Epoch 66, loss=[[0.69305073]]\n",
      "Epoch 67, loss=[[0.69305455]]\n",
      "Epoch 68, loss=[[0.69304843]]\n",
      "Epoch 69, loss=[[0.69305284]]\n",
      "Epoch 70, loss=[[0.69304531]]\n",
      "Epoch 71, loss=[[0.69305279]]\n",
      "Epoch 72, loss=[[0.69304765]]\n",
      "Epoch 73, loss=[[0.69304972]]\n",
      "Epoch 74, loss=[[0.69304825]]\n",
      "Epoch 75, loss=[[0.69305603]]\n",
      "Epoch 76, loss=[[0.69304971]]\n",
      "Epoch 77, loss=[[0.69303999]]\n",
      "Epoch 78, loss=[[0.69304407]]\n",
      "Epoch 79, loss=[[0.69305039]]\n",
      "Epoch 80, loss=[[0.69304042]]\n",
      "Epoch 81, loss=[[0.69304046]]\n",
      "Epoch 82, loss=[[0.69304006]]\n",
      "Epoch 83, loss=[[0.69304366]]\n",
      "Epoch 84, loss=[[0.69304089]]\n",
      "Epoch 85, loss=[[0.69304378]]\n",
      "Epoch 86, loss=[[0.69305072]]\n",
      "Epoch 87, loss=[[0.69304693]]\n",
      "Epoch 88, loss=[[0.69304127]]\n",
      "Epoch 89, loss=[[0.69303516]]\n",
      "Epoch 90, loss=[[0.6930411]]\n",
      "Epoch 91, loss=[[0.69303442]]\n",
      "Epoch 92, loss=[[0.69303907]]\n",
      "Epoch 93, loss=[[0.69304228]]\n",
      "Epoch 94, loss=[[0.69304456]]\n",
      "Epoch 95, loss=[[0.69304178]]\n",
      "Epoch 96, loss=[[0.69304267]]\n",
      "Epoch 97, loss=[[0.69303906]]\n",
      "Epoch 98, loss=[[0.69304288]]\n",
      "Epoch 99, loss=[[0.6930435]]\n",
      "Epoch 100, loss=[[0.69303162]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "raw = pd.read_csv('day.csv')\n",
    "raw[\"success\"] = raw[\"cnt\"] > (raw[\"cnt\"].describe()[\"mean\"])\n",
    "\n",
    "X = raw[['temp', 'atemp', 'hum', 'windspeed', 'weekday']].values\n",
    "y = raw[\"success\"].values\n",
    "\n",
    "nn = MyNN(0.001, [5, 40, 30, 10, 7, 5, 3, 1])\n",
    "batch_size = 8\n",
    "epochs = 100\n",
    "\n",
    "loss = train(X.T, y.reshape(1,-1), epochs, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(epochs),loss)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "DL Assignment 1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
